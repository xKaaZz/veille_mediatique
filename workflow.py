from llama_index.core.workflow import (
    StartEvent,
    StopEvent,
    Workflow,
    step,
    Event,
)
from database_manager import DatabaseManager
from rss_scraper import RSSScraper
from embedding_generator import EmbeddingGenerator
from search_embeddings import SearchEmbeddings
from news_processing_utils import summarize_cluster, generate_cluster_label, send_telegram_message
from mongo_docstore import MongoDBDocStore
from datetime import date
import numpy as np
import os
from sklearn.cluster import DBSCAN
from collections import defaultdict
from dotenv import load_dotenv
from llama_index.utils.workflow import draw_all_possible_flows
from datetime import date, timedelta

load_dotenv()

# D√©finition des √©v√©nements
class ArticlesScraped(Event):
    articles: list

class ArticlesIndexed(Event):
    articles: list

class ArticlesClustered(Event):
    clusters: dict

class ClustersLabeled(Event):
    labeled_clusters: dict

class ArticlesSummarized(Event):
    summaries: dict

# Classe principale du workflow
class NewsProcessingWorkflow(Workflow):
    def __init__(self, duration):  
        self.db_manager = DatabaseManager()
        self.scraper = RSSScraper(self.db_manager)
        self.docstore = MongoDBDocStore(
            uri=os.getenv("MONGO_URI"),
            db_name=os.getenv("MONGO_DB_NAME"),
            collection_name=os.getenv("MONGO_COLLECTION")
        )
        self.embedder = EmbeddingGenerator(self.db_manager, self.docstore, os.getenv("OPENAI_API_KEY"))
        self.duration = duration  
        

    @step
    async def scrape_articles(self, ev: StartEvent) -> ArticlesScraped:
        print("üì° Scraping des flux RSS en cours...")

        feeds_with_categories = self.scraper.get_rss_feeds_with_categories()
        if not feeds_with_categories:
            print("‚ùå Aucun flux RSS enregistr√© en base !")
            return ArticlesScraped(articles=[])

        for feed_url, category in feeds_with_categories.items():
            print(f"üì° Scraping {feed_url} (Cat√©gorie : {category})")
            self.scraper.scrape_feed(feed_url, category)

        articles = self.docstore.get_all_documents()
        print(f"üìå Articles √† indexer : {len(articles)}")
        return ArticlesScraped(articles=articles)

    @step
    async def index_articles(self, ev: ArticlesScraped) -> ArticlesIndexed:
        print("üîç G√©n√©ration des embeddings...")

        articles_to_update = [
            article for article in ev.articles 
            if not article.get("content_vector")
        ]

        print(f"üìå Articles n√©cessitant un embedding : {len(articles_to_update)}")

        for article in articles_to_update:
            text = f"{article['title']} {article.get('content', '')}"
            embedding = self.embedder.generate_embedding(text)
            if embedding:
                self.docstore.update_document(article["_id"], {"content_vector": embedding})

        updated_articles = self.docstore.get_all_documents()
        return ArticlesIndexed(articles=updated_articles)

    @step
    async def refine_article_categories(self, ev: ArticlesIndexed) -> ArticlesClustered:
        print(f"üß© V√©rification et ajustement des cat√©gories des articles publi√©s ces {self.duration} derniers jours...")

        # Calcul de la date de d√©but en fonction de la dur√©e
        start_date = date.today() - timedelta(days=self.duration)

        # R√©cup√©ration des articles selon cette p√©riode
        articles = self.db_manager.get_articles_since(start_date)

        # Log pour v√©rification
        print(f"üìÖ Nombre d'articles r√©cup√©r√©s : {len(articles)}")
        categories = defaultdict(list)

        for article in articles:
            category = article.get("category", "uncategorized")
            categories[category].append(article)

        updated_clusters = {}

        for category, cat_articles in categories.items():
            print(f"üìå Analyse de la cat√©gorie : {category} ({len(cat_articles)} articles)")

            # V√©rification qu'on a assez d'articles
            if len(cat_articles) < 3:
                print(f"‚ö†Ô∏è Pas assez d'articles dans {category} pour un clustering pertinent.")
                updated_clusters[category] = {"0": cat_articles}  # ‚úÖ Fixe le format de la structure
                continue  

            embeddings = np.array([article["content_vector"] for article in cat_articles if article.get("content_vector")])

            if embeddings.shape[0] < 3:  # ‚úÖ V√©rification qu'on a assez d'embeddings valides
                print(f"‚ö†Ô∏è Pas assez de donn√©es pour clusteriser {category}.")
                updated_clusters[category] = {"0": cat_articles}
                continue

            # üî• Clustering avec DBSCAN (on garde un eps raisonnable)
            clustering = DBSCAN(eps=0.2, min_samples=4, metric='cosine').fit(embeddings)

            category_clusters = defaultdict(list)
            isolated_articles = []

            for label, article in zip(clustering.labels_, cat_articles):
                if label == -1:
                    isolated_articles.append(article)
                else:
                    category_clusters[label].append(article)

            updated_clusters[category] = {str(label): articles for label, articles in category_clusters.items()}

            # üìå V√©rification des articles isol√©s
            for article in isolated_articles:
                best_category = None
                best_distance = float("inf")

                for other_category, other_articles in categories.items():
                    if other_category == category or len(other_articles) < 3:
                        continue
                    
                    other_embeddings = np.array([a["content_vector"] for a in other_articles if a.get("content_vector")])

                    if other_embeddings.shape[0] < 3:
                        continue

                    avg_distance = np.mean([np.linalg.norm(article["content_vector"] - emb) for emb in other_embeddings])

                    if avg_distance < best_distance:
                        best_distance = avg_distance
                        best_category = other_category

                if best_category and best_distance < 0.2:
                    print(f"üîÑ Changement de cat√©gorie : {article['title']} passe de {category} √† {best_category}")
                    if "0" not in updated_clusters[best_category]:
                        updated_clusters[best_category]["0"] = []  # üõ†Ô∏è Initialisation correcte
                    updated_clusters[best_category]["0"].append(article)
                else:
                    print(f"‚ùå {article['title']} reste dans {category}.")
                    if "0" not in updated_clusters[category]:  
                        updated_clusters[category]["0"] = []  # üõ†Ô∏è Initialisation correcte
                    updated_clusters[category]["0"].append(article)

        print(f"‚úÖ V√©rification des cat√©gories termin√©e avec {len(updated_clusters)} cat√©gories mises √† jour.")
        return ArticlesClustered(clusters=updated_clusters)

    @step
    async def label_clusters(self, ev: ArticlesClustered) -> ClustersLabeled:
        print("üè∑Ô∏è G√©n√©ration des labels pour chaque cluster...")

        labeled_clusters = {}
        
        for category, category_clusters in ev.clusters.items():
            for cluster_id, articles in category_clusters.items():
                titles = [article["title"] for article in articles]
                label = generate_cluster_label(titles)
                
                labeled_clusters[(category, cluster_id)] = {
                    "label": label,
                    "category": category,
                    "articles": articles
                }

        print(f"‚úÖ Labels g√©n√©r√©s pour {len(labeled_clusters)} clusters.")
        return ClustersLabeled(labeled_clusters=labeled_clusters)

    @step
    async def summarize_clusters(self, ev: ClustersLabeled) -> ArticlesSummarized:
        print("üìù G√©n√©ration des r√©sum√©s pour chaque cluster...")

        summaries = {}
        for (category, cluster_id), cluster_info in ev.labeled_clusters.items():
            label = cluster_info["label"]
            articles = cluster_info["articles"]
            summary = summarize_cluster(articles)
            summaries[f"{category} - {label}"] = summary

        return ArticlesSummarized(summaries=summaries)

    @step
    async def finalize_workflow(self, ev: ArticlesSummarized) -> StopEvent:
        print("üìÖ Synth√®se des faits marquants du jour par th√®me :")

        def split_message(text, max_length=4096):
            """Divise un texte en morceaux compatibles avec la limite de caract√®res de Telegram."""
            messages = []
            while len(text) > max_length:
                split_index = text[:max_length].rfind('\n')  # Couper proprement √† la fin d'une ligne si possible
                if split_index == -1:
                    split_index = max_length  # Au pire, couper au maximum autoris√©
                messages.append(text[:split_index])
                text = text[split_index:].strip()
            if text:
                messages.append(text)
            return messages

        for cluster_name, summary in ev.summaries.items():
            category, label = cluster_name.split(" - ", 1)
            message = f"\nüåç {category.capitalize()} : {label}\n{summary}"

            for chunk in split_message(message):
                try:
                    # Utilise ton code d'envoi de message ici (ajuste en fonction de ton impl√©mentation Telegram)
                    send_telegram_message(chunk)
                except Exception as e:
                    print(f"‚ùå Erreur lors de l'envoi du message : {e}")

        return StopEvent(result="‚úÖ Workflow termin√© avec succ√®s !")



# Cr√©ation du workflow
news_workflow = NewsProcessingWorkflow()
#draw_all_possible_flows(NewsProcessingWorkflow, filename="NewsProcessingWorkflow.html")