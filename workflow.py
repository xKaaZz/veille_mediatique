from llama_index.core.workflow import (
    StartEvent,
    StopEvent,
    Workflow,
    step,
    Event,
)
from database_manager import DatabaseManager
from rss_scraper import RSSScraper
from embedding_generator import EmbeddingGenerator
from search_embeddings import SearchEmbeddings
from news_processing_utils import summarize_cluster, generate_cluster_label
from datetime import date
import numpy as np
import os
from sklearn.cluster import DBSCAN
from collections import defaultdict
from dotenv import load_dotenv

load_dotenv()

# DÃ©finition des Ã©vÃ©nements
class ArticlesScraped(Event):
    articles: list

class ArticlesIndexed(Event):
    articles: list

class ArticlesClustered(Event):
    clusters: dict

class ClustersLabeled(Event):
    labeled_clusters: dict

class ArticlesSummarized(Event):
    summaries: dict

# Classe principale du workflow
class NewsProcessingWorkflow(Workflow):
    def __init__(self):
        super().__init__(timeout=60, verbose=True)
        self.db_manager = DatabaseManager()
        self.scraper = RSSScraper(self.db_manager)
        self.embedder = EmbeddingGenerator(self.db_manager, os.getenv("OPENAI_API_KEY"))

    @step
    async def scrape_articles(self, ev: StartEvent) -> ArticlesScraped:
        print("ğŸ“¡ Scraping des flux RSS en cours...")
        
        feeds_with_categories = self.scraper.get_rss_feeds_with_categories()
        if not feeds_with_categories:
            print("âŒ Aucun flux RSS enregistrÃ© en base !")
            return ArticlesScraped(articles=[])

        for feed_url, category in feeds_with_categories.items():
            print(f"ğŸ“¡ Scraping {feed_url} (CatÃ©gorie : {category})")
            self.scraper.scrape_feed(feed_url, category)

        articles = self.db_manager.get_articles_of_day(date.today())
        print(f"ğŸ“Œ Articles Ã  indexer : {len(articles)}")
        return ArticlesScraped(articles=articles)

    @step
    async def index_articles(self, ev: ArticlesScraped) -> ArticlesIndexed:
        print("ğŸ” GÃ©nÃ©ration des embeddings...")

        articles_to_update = [
            article for article in ev.articles 
            if not article.get("content_vector")  # âœ… Ã‰vite les `None`
        ]
        
        print(f"ğŸ“Œ Articles nÃ©cessitant un embedding : {len(articles_to_update)}")
        
        for article in articles_to_update:
            text = f"{article['title']} {article.get('content', '')}"
            embedding = self.embedder.generate_embedding(text)
            if embedding:
                self.db_manager.update_embedding(article["link"], embedding)

        updated_articles = self.db_manager.get_articles_of_day(date.today())
        return ArticlesIndexed(articles=updated_articles)

    @step
    async def refine_article_categories(self, ev: ArticlesIndexed) -> ArticlesClustered:
        print("ğŸ§© VÃ©rification et ajustement des catÃ©gories des articles...")

        articles = self.db_manager.get_articles_of_day(date.today())
        categories = defaultdict(list)

        for article in articles:
            category = article.get("category", "uncategorized")
            categories[category].append(article)

        updated_clusters = {}

        for category, cat_articles in categories.items():
            print(f"ğŸ“Œ Analyse de la catÃ©gorie : {category} ({len(cat_articles)} articles)")

            # VÃ©rification qu'on a assez d'articles
            if len(cat_articles) < 3:
                print(f"âš ï¸ Pas assez d'articles dans {category} pour un clustering pertinent.")
                updated_clusters[category] = {"0": cat_articles}  # âœ… Fixe le format de la structure
                continue  

            embeddings = np.array([article["content_vector"] for article in cat_articles if article.get("content_vector")])

            if embeddings.shape[0] < 3:  # âœ… VÃ©rification qu'on a assez d'embeddings valides
                print(f"âš ï¸ Pas assez de donnÃ©es pour clusteriser {category}.")
                updated_clusters[category] = {"0": cat_articles}
                continue

            # ğŸ”¥ Clustering avec DBSCAN (on garde un eps raisonnable)
            clustering = DBSCAN(eps=0.15, min_samples=3, metric='cosine').fit(embeddings)

            category_clusters = defaultdict(list)
            isolated_articles = []

            for label, article in zip(clustering.labels_, cat_articles):
                if label == -1:
                    isolated_articles.append(article)
                else:
                    category_clusters[label].append(article)

            updated_clusters[category] = {str(label): articles for label, articles in category_clusters.items()}

            # ğŸ“Œ VÃ©rification des articles isolÃ©s
            for article in isolated_articles:
                best_category = None
                best_distance = float("inf")

                for other_category, other_articles in categories.items():
                    if other_category == category or len(other_articles) < 3:
                        continue
                    
                    other_embeddings = np.array([a["content_vector"] for a in other_articles if a.get("content_vector")])

                    if other_embeddings.shape[0] < 3:
                        continue

                    avg_distance = np.mean([np.linalg.norm(article["content_vector"] - emb) for emb in other_embeddings])

                    if avg_distance < best_distance:
                        best_distance = avg_distance
                        best_category = other_category

                if best_category and best_distance < 0.3:
                    print(f"ğŸ”„ Changement de catÃ©gorie : {article['title']} passe de {category} Ã  {best_category}")
                    updated_clusters[best_category]["0"].append(article)
                else:
                    print(f"âŒ {article['title']} reste dans {category}.")
                    updated_clusters[category]["0"].append(article)

        print(f"âœ… VÃ©rification des catÃ©gories terminÃ©e avec {len(updated_clusters)} catÃ©gories mises Ã  jour.")
        return ArticlesClustered(clusters=updated_clusters)

    @step
    async def label_clusters(self, ev: ArticlesClustered) -> ClustersLabeled:
        print("ğŸ·ï¸ GÃ©nÃ©ration des labels pour chaque cluster...")

        labeled_clusters = {}
        
        for category, category_clusters in ev.clusters.items():
            for cluster_id, articles in category_clusters.items():
                titles = [article["title"] for article in articles]
                label = generate_cluster_label(titles)
                
                labeled_clusters[(category, cluster_id)] = {
                    "label": label,
                    "category": category,
                    "articles": articles
                }

        print(f"âœ… Labels gÃ©nÃ©rÃ©s pour {len(labeled_clusters)} clusters.")
        return ClustersLabeled(labeled_clusters=labeled_clusters)

    @step
    async def summarize_clusters(self, ev: ClustersLabeled) -> ArticlesSummarized:
        print("ğŸ“ GÃ©nÃ©ration des rÃ©sumÃ©s pour chaque cluster...")

        summaries = {}
        for (category, cluster_id), cluster_info in ev.labeled_clusters.items():
            label = cluster_info["label"]
            articles = cluster_info["articles"]
            summary = summarize_cluster(articles)
            summaries[f"{category} - {label}"] = summary

        return ArticlesSummarized(summaries=summaries)

    @step
    async def finalize_workflow(self, ev: ArticlesSummarized) -> StopEvent:
        print("ğŸ“… SynthÃ¨se des faits marquants du jour par thÃ¨me :")

        for cluster_name, summary in ev.summaries.items():
            category, label = cluster_name.split(" - ", 1)
            print(f"\nğŸŒ {category.capitalize()} : {label}")
            print(summary)

        return StopEvent(result="âœ… Workflow terminÃ© avec succÃ¨s !")


# CrÃ©ation du workflow
news_workflow = NewsProcessingWorkflow()
